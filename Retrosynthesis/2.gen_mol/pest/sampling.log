08:47:25 <INFO> Started REINVENT 4.4.22 (C) AstraZeneca 2017, 2023 on 2024-08-24
08:47:25 <INFO> Command line: /home/zh/anaconda3/envs/reinvent4/bin/reinvent -l sampling.log sampling.toml
08:47:25 <INFO> User zh on host LZD-20220629LJL
08:47:25 <INFO> Python version 3.10.14
08:47:25 <INFO> PyTorch version 2.2.1+cu121, git 6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0
08:47:25 <INFO> PyTorch compiled with CUDA version 12.1
08:47:25 <INFO> RDKit version 2023.09.5
08:47:25 <INFO> Platform Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31
08:47:25 <INFO> Number of PyTorch CUDA devices 0
08:47:25 <INFO> Using CPU x86_64
08:47:25 <INFO> Writing JSON config file to /home/zh/REINVENT4/gen/undel/pest/_sampling.json
08:47:25 <INFO> Starting Sampling
08:47:26 <INFO> /home/zh/REINVENT4/gen/undel/pest/TL_reinvent.model.12.chkpt has valid hash:
{ 'comments': ['TL'],
  'creation_date': 0,
  'date_format': 'UNIX epoch',
  'hash_id': 'f321de133be792ba6ba1d4f8868edd2e',
  'hash_id_format': 'xxhash.xxh3_128_hex 3.4.1',
  'model_id': 'dbe73d69456f42f990b955233dc7f514',
  'model_id_format': 'uuid.uuid4 3.10.12',
  'origina_data_source': 'ChEMBL 28',
  'updates': [1724408099.8886697]}
08:47:26 <INFO> Number of network parameters: 17,462,400
08:47:26 <INFO> Network architecture:
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0-5): 6 x DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0-2): 3 x SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (src_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(128, 256)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (tgt_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(128, 256)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (generator): Generator(
    (proj): Linear(in_features=256, out_features=128, bias=True)
  )
)
08:47:26 <INFO> Using generator Mol2Mol
08:47:26 <INFO> Writing sampled SMILES to CSV file sampling.csv
08:47:26 <WARN> randomize_smiles is set to be True by user. But the model was trained using canonical SMILESwhere randomize_smiles might undermine the performance (this needs more investigation), but randomize_smiles is reset to be False for now.
08:47:26 <INFO> Sampling 4905 SMILES from model TL_reinvent.model.12.chkpt
09:17:47 <INFO> Peak main memory usage: 1964.852 MiB
09:17:47 <INFO> Finished REINVENT on 2024-08-24
